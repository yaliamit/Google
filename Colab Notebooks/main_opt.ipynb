{"nbformat":4,"nbformat_minor":0,"metadata":{"accelerator":"GPU","colab":{"name":"main_opt.ipynb","provenance":[{"file_id":"1WmUxJR7iHPVztDlbHikwdhEF864hkBwt","timestamp":1590781851210}]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.4"}},"cells":[{"cell_type":"code","metadata":{"colab_type":"code","id":"jddndr46m_P2","scrolled":true,"colab":{"base_uri":"https://localhost:8080/","height":176},"executionInfo":{"status":"ok","timestamp":1599762367623,"user_tz":300,"elapsed":18114,"user":{"displayName":"Yali Amit","photoUrl":"","userId":"06843479468473845794"}},"outputId":"b54a4a6e-7c86-4a50-d3b9-626df523848d"},"source":["import os\n","import sys\n","if 'Linux' in os.uname():\n","    from google.colab import drive\n","    drive.mount('/ME')\n","    predir='/ME/My Drive/'\n","else:\n","    predir='/Users/amit/Google Drive/'\n","    \n","from importlib import reload\n","!pip install ninja\n","datadirs=predir+'Colab Notebooks/STVAE/'\n","sys.path.insert(1, datadirs)\n","sys.path.insert(1, datadirs+'_CODE')\n","import torch\n","import model_layers\n","from model_layers import FALinear, FAConv2d\n","from aux_colab import seq, copy_to_content, copy_from_content, save_net, train_net, run_net\n","\n","device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Drive already mounted at /ME; to attempt to forcibly remount, call drive.mount(\"/ME\", force_remount=True).\n","Requirement already satisfied: ninja in /usr/local/lib/python3.6/dist-packages (1.10.0.post2)\n","Using /root/.cache/torch_extensions as PyTorch extensions root...\n","Creating extension directory /root/.cache/torch_extensions/cudnn_convolution...\n","Emitting ninja build file /root/.cache/torch_extensions/cudnn_convolution/build.ninja...\n","Building extension module cudnn_convolution...\n","Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n","Loading extension module cudnn_convolution...\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"7Rmlpo-JwoiJ","colab_type":"code","colab":{}},"source":["copy_to_content('pars_emb_cifar',predir)\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"dcM1JV5SMZVn","colab_type":"code","colab":{}},"source":["net=run_net('pars_emb_cifar',device)\n","save_net(net,'pars_emb_cifar',predir)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"CFJ1fHcXiLIz","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":1000},"outputId":"6668663a-edb2-4629-f3b4-c2eb6da7443b"},"source":["seq('pars_emb_cifar',predir,device)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["OUT_pool1_emb.txt\n","/ME/My Drive/LSDA_data/CIFAR/cifar100_train.hdf5\n","tr (50000, 32, 32, 3)\n","n_classes 2.0 dim 32 nchannels 3\n","In get_data_pre: num_train 50000\n","Num test: 10000\n","NUMCLASS 100\n","ModuleList(\n","  (conv1): FAConv2d(3, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), bias=False)\n","  (pool1): MaxPool2d(kernel_size=2, stride=2, padding=[0], dilation=1, ceil_mode=False)\n","  (dense_final): FALinear(in_features=8192, out_features=64, bias=False)\n",")\n","getting:cifar10\n","/ME/My Drive/LSDA_data/CIFAR/cifar10_train.hdf5\n","tr (50000, 32, 32, 3)\n","n_classes 2.0 dim 32 nchannels 3\n","In get_data_pre: num_train 50000\n","Num test: 10000\n","In from hidden number of training 50000\n","In train new:\n","Namespace(CONS=False, Diag=False, MM=False, OPT=False, binary_thresh=1e-06, cl=None, classify=False, clust=None, conf=0, cont_training=False, datadirs='/ME/My Drive/Colab Notebooks/STVAE/', dataset='cifar100', del_last=False, edge_dtr=0.0, edges=False, embedd=False, embedd_layer='pool1', embedd_type='L1dist_hinge', enc_layers=None, erode=False, fa=0, feats=0, feats_back=False, filts=3, fout=<_io.TextIOWrapper name='/ME/My Drive/Colab Notebooks/STVAE/OUT_pool1_emb.txt' mode='a+' encoding='UTF-8'>, full_dim=256, gpu=1, h_factor=0.2, hdim=256, hid_dataset='cifar10', hid_drop=0.0, hid_hid=256, hid_layers=['name:input', 'name:dense_final_hid;num_units:10'], hid_lr=0.001, hid_nepoch=400, hinge=False, input_channels=1, lamda=1.0, lamda1=1.0, layers=['name:input;num_filters:3', 'name:conv1;num_filters:32;filter_size:5;non_linearity:relu;nb:0', 'name:pool1;pool_size:2', 'name:dense_final;num_units:64;nb:0;parent:[pool1]'], layers_dict=[{'name': 'input', 'num_filters': 3}, {'name': 'conv1', 'num_filters': 32, 'filter_size': 5, 'non_linearity': 'relu', 'nb': 0}, {'name': 'pool1', 'pool_size': 2}, {'name': 'dense_final', 'num_units': 64, 'nb': 0, 'parent': ['pool1']}], layerwise=True, lim=0, lnti={'input': 0, 'conv1': 1, 'pool1': 2, 'dense_final': 3}, lr=0.001, mb_size=500, model=None, model_out='network_pool1_emb', mu_lr=[0.05, 0.01], n_class=0, n_mix=0, n_part_locs=0, n_parts=0, nepoch=400, network=True, network_num_train=50000, nti=500, num_class=10, num_mu_iter=10, num_test=0, num_train=50000, nval=0, nvi=20, only_pi=False, opt_jump=1, optimizer='Adam', ortho_lr=0.0, out_file='OUT_pool1_emb.txt', output_cont=False, output_prefix='', par_file='t_par', part_dim=None, perturb=0.4, pool=2, pool_stride=2, reinit=False, rerun=False, run_existing=False, s_factor=4.0, sample=False, scale=None, sched=0, sdim=26, seed=1111, sep=False, tps_num=3, transformation='aff', type='net', update_layers=None, wd=0)\n","input [   1 8192]\n","dense_final_hid [ 1 10]\n","ModuleList(\n","  (dense_final_hid): FALinear(in_features=8192, out_features=10, bias=True)\n",")\n","layers.dense_final_hid.weight,[  10 8192]\n","layers.dense_final_hid.weight_fb,[  10 8192]\n","layers.dense_final_hid.bias,[10]\n","tot_pars,163850\n","TO optimizer layers.dense_final_hid.weight[  10 8192]\n","TO optimizer layers.dense_final_hid.weight_fb[  10 8192]\n","TO optimizer layers.dense_final_hid.bias[10]\n","Optimizer Adam 0.001\n","DONE\n","network_pool1_emb\n","OUT_pool2_emb.txt\n","/ME/My Drive/LSDA_data/CIFAR/cifar100_train.hdf5\n","tr (50000, 32, 32, 3)\n","n_classes 2.0 dim 32 nchannels 3\n","In get_data_pre: num_train 50000\n","Num test: 10000\n","NUMCLASS 100\n","ModuleList(\n","  (conv1): FAConv2d(3, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), bias=False)\n","  (pool1): MaxPool2d(kernel_size=2, stride=2, padding=[0], dilation=1, ceil_mode=False)\n","  (conv2): FAConv2d(32, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), bias=False)\n","  (pool2): MaxPool2d(kernel_size=2, stride=2, padding=[0], dilation=1, ceil_mode=False)\n","  (dense_final): FALinear(in_features=4096, out_features=64, bias=False)\n",")\n","In reinit, cont training: False\n","getting:cifar10\n","/ME/My Drive/LSDA_data/CIFAR/cifar10_train.hdf5\n","tr (50000, 32, 32, 3)\n","n_classes 2.0 dim 32 nchannels 3\n","In get_data_pre: num_train 50000\n","Num test: 10000\n","In from hidden number of training 50000\n","In train new:\n","Namespace(CONS=False, Diag=False, MM=False, OPT=False, binary_thresh=1e-06, cl=None, classify=False, clust=None, conf=0, cont_training=False, datadirs='/ME/My Drive/Colab Notebooks/STVAE/', dataset='cifar100', del_last=False, edge_dtr=0.0, edges=False, embedd=False, embedd_layer='pool2', embedd_type='L1dist_hinge', enc_layers=None, erode=False, fa=0, feats=0, feats_back=False, filts=3, fout=<_io.TextIOWrapper name='/ME/My Drive/Colab Notebooks/STVAE/OUT_pool2_emb.txt' mode='a+' encoding='UTF-8'>, full_dim=256, gpu=1, h_factor=0.2, hdim=256, hid_dataset='cifar10', hid_drop=0.0, hid_hid=256, hid_layers=['name:input', 'name:dense_final_hid;num_units:10'], hid_lr=0.001, hid_nepoch=400, hinge=False, input_channels=1, lamda=1.0, lamda1=1.0, layers=['name:input;num_filters:3', 'name:conv1;num_filters:32;filter_size:5;non_linearity:relu;nb:0', 'name:pool1;pool_size:2', '#name:drop1;drop:.5', 'name:conv2;num_filters:64;filter_size:5;non_linearity:relu;nb:0', 'name:pool2;pool_size:2', 'name:dense_final;num_units:64;nb:0;parent:[pool2]'], layers_dict=[{'name': 'input', 'num_filters': 3}, {'name': 'conv1', 'num_filters': 32, 'filter_size': 5, 'non_linearity': 'relu', 'nb': 0}, {'name': 'pool1', 'pool_size': 2}, {'name': 'conv2', 'num_filters': 64, 'filter_size': 5, 'non_linearity': 'relu', 'nb': 0}, {'name': 'pool2', 'pool_size': 2}, {'name': 'dense_final', 'num_units': 64, 'nb': 0, 'parent': ['pool2']}], layerwise=True, lim=0, lnti={'input': 0, 'conv1': 1, 'pool1': 2, 'conv2': 3, 'pool2': 4, 'dense_final': 5}, lr=0.001, mb_size=500, model=['network_pool1_emb'], model_out='network_pool2_emb', mu_lr=[0.05, 0.01], n_class=0, n_mix=0, n_part_locs=0, n_parts=0, nepoch=400, network=True, network_num_train=50000, nti=500, num_class=10, num_mu_iter=10, num_test=0, num_train=50000, nval=0, nvi=20, only_pi=False, opt_jump=1, optimizer='Adam', ortho_lr=0.0, out_file='OUT_pool2_emb.txt', output_cont=False, output_prefix='', par_file='t_par', part_dim=None, perturb=0.4, pool=2, pool_stride=2, reinit=True, rerun=False, run_existing=True, s_factor=4.0, sample=False, scale=None, sched=0, sdim=26, seed=1111, sep=False, tps_num=3, transformation='aff', type='net', update_layers=None, wd=0)\n","input [   1 4096]\n","dense_final_hid [ 1 10]\n","ModuleList(\n","  (dense_final_hid): FALinear(in_features=4096, out_features=10, bias=True)\n",")\n","layers.dense_final_hid.weight,[  10 4096]\n","layers.dense_final_hid.weight_fb,[  10 4096]\n","layers.dense_final_hid.bias,[10]\n","tot_pars,81930\n","TO optimizer layers.dense_final_hid.weight[  10 4096]\n","TO optimizer layers.dense_final_hid.weight_fb[  10 4096]\n","TO optimizer layers.dense_final_hid.bias[10]\n","Optimizer Adam 0.001\n","DONE\n","network_pool2_emb\n","OUT_pool3_emb.txt\n","/ME/My Drive/LSDA_data/CIFAR/cifar100_train.hdf5\n","tr (50000, 32, 32, 3)\n","n_classes 2.0 dim 32 nchannels 3\n","In get_data_pre: num_train 50000\n","Num test: 10000\n","NUMCLASS 100\n","ModuleList(\n","  (conv1): FAConv2d(3, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), bias=False)\n","  (pool1): MaxPool2d(kernel_size=2, stride=2, padding=[0], dilation=1, ceil_mode=False)\n","  (conv2): FAConv2d(32, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), bias=False)\n","  (pool2): MaxPool2d(kernel_size=2, stride=2, padding=[0], dilation=1, ceil_mode=False)\n","  (conv3): FAConv2d(64, 128, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), bias=False)\n","  (pool3): MaxPool2d(kernel_size=2, stride=2, padding=[0], dilation=1, ceil_mode=False)\n","  (dense_final): FALinear(in_features=2048, out_features=64, bias=False)\n",")\n","In reinit, cont training: False\n","getting:cifar10\n","/ME/My Drive/LSDA_data/CIFAR/cifar10_train.hdf5\n","tr (50000, 32, 32, 3)\n","n_classes 2.0 dim 32 nchannels 3\n","In get_data_pre: num_train 50000\n","Num test: 10000\n","In from hidden number of training 50000\n","In train new:\n","Namespace(CONS=False, Diag=False, MM=False, OPT=False, binary_thresh=1e-06, cl=None, classify=False, clust=None, conf=0, cont_training=False, datadirs='/ME/My Drive/Colab Notebooks/STVAE/', dataset='cifar100', del_last=False, edge_dtr=0.0, edges=False, embedd=False, embedd_layer='pool3', embedd_type='L1dist_hinge', enc_layers=None, erode=False, fa=0, feats=0, feats_back=False, filts=3, fout=<_io.TextIOWrapper name='/ME/My Drive/Colab Notebooks/STVAE/OUT_pool3_emb.txt' mode='a+' encoding='UTF-8'>, full_dim=256, gpu=1, h_factor=0.2, hdim=256, hid_dataset='cifar10', hid_drop=0.0, hid_hid=256, hid_layers=['name:input', 'name:dense_final_hid;num_units:10'], hid_lr=0.001, hid_nepoch=400, hinge=False, input_channels=1, lamda=1.0, lamda1=1.0, layers=['name:input;num_filters:3', 'name:conv1;num_filters:32;filter_size:5;non_linearity:relu;nb:0', 'name:pool1;pool_size:2', '#name:drop1;drop:.5', 'name:conv2;num_filters:64;filter_size:5;non_linearity:relu;nb:0', 'name:pool2;pool_size:2', '#name:drop2;drop:.5', 'name:conv3;num_filters:128;filter_size:5;non_linearity:relu;nb:0', 'name:pool3;pool_size:2', 'name:dense_final;num_units:64;nb:0;parent:[pool3]'], layers_dict=[{'name': 'input', 'num_filters': 3}, {'name': 'conv1', 'num_filters': 32, 'filter_size': 5, 'non_linearity': 'relu', 'nb': 0}, {'name': 'pool1', 'pool_size': 2}, {'name': 'conv2', 'num_filters': 64, 'filter_size': 5, 'non_linearity': 'relu', 'nb': 0}, {'name': 'pool2', 'pool_size': 2}, {'name': 'conv3', 'num_filters': 128, 'filter_size': 5, 'non_linearity': 'relu', 'nb': 0}, {'name': 'pool3', 'pool_size': 2}, {'name': 'dense_final', 'num_units': 64, 'nb': 0, 'parent': ['pool3']}], layerwise=True, lim=0, lnti={'input': 0, 'conv1': 1, 'pool1': 2, 'conv2': 3, 'pool2': 4, 'conv3': 5, 'pool3': 6, 'dense_final': 7}, lr=0.001, mb_size=500, model=['network_pool2_emb'], model_out='network_pool3_emb', mu_lr=[0.05, 0.01], n_class=0, n_mix=0, n_part_locs=0, n_parts=0, nepoch=400, network=True, network_num_train=50000, nti=500, num_class=10, num_mu_iter=10, num_test=0, num_train=50000, nval=0, nvi=20, only_pi=False, opt_jump=1, optimizer='Adam', ortho_lr=0.0, out_file='OUT_pool3_emb.txt', output_cont=False, output_prefix='', par_file='t_par', part_dim=None, perturb=0.4, pool=2, pool_stride=2, reinit=True, rerun=False, run_existing=True, s_factor=4.0, sample=False, scale=None, sched=0, sdim=26, seed=1111, sep=False, tps_num=3, transformation='aff', type='net', update_layers=None, wd=0)\n","input [   1 2048]\n","dense_final_hid [ 1 10]\n","ModuleList(\n","  (dense_final_hid): FALinear(in_features=2048, out_features=10, bias=True)\n",")\n","layers.dense_final_hid.weight,[  10 2048]\n","layers.dense_final_hid.weight_fb,[  10 2048]\n","layers.dense_final_hid.bias,[10]\n","tot_pars,40970\n","TO optimizer layers.dense_final_hid.weight[  10 2048]\n","TO optimizer layers.dense_final_hid.weight_fb[  10 2048]\n","TO optimizer layers.dense_final_hid.bias[10]\n","Optimizer Adam 0.001\n","DONE\n","network_pool3_emb\n","OUT_conv4_emb.txt\n","/ME/My Drive/LSDA_data/CIFAR/cifar100_train.hdf5\n","tr (50000, 32, 32, 3)\n","n_classes 2.0 dim 32 nchannels 3\n","In get_data_pre: num_train 50000\n","Num test: 10000\n","NUMCLASS 100\n","ModuleList(\n","  (conv1): FAConv2d(3, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), bias=False)\n","  (pool1): MaxPool2d(kernel_size=2, stride=2, padding=[0], dilation=1, ceil_mode=False)\n","  (conv2): FAConv2d(32, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), bias=False)\n","  (pool2): MaxPool2d(kernel_size=2, stride=2, padding=[0], dilation=1, ceil_mode=False)\n","  (conv3): FAConv2d(64, 128, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), bias=False)\n","  (pool3): MaxPool2d(kernel_size=2, stride=2, padding=[0], dilation=1, ceil_mode=False)\n","  (conv4): FAConv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","  (dense_final): FALinear(in_features=4096, out_features=64, bias=False)\n",")\n","In reinit, cont training: False\n","getting:cifar10\n","/ME/My Drive/LSDA_data/CIFAR/cifar10_train.hdf5\n","tr (50000, 32, 32, 3)\n","n_classes 2.0 dim 32 nchannels 3\n","In get_data_pre: num_train 50000\n","Num test: 10000\n","In from hidden number of training 50000\n","In train new:\n","Namespace(CONS=False, Diag=False, MM=False, OPT=False, binary_thresh=1e-06, cl=None, classify=False, clust=None, conf=0, cont_training=False, datadirs='/ME/My Drive/Colab Notebooks/STVAE/', dataset='cifar100', del_last=False, edge_dtr=0.0, edges=False, embedd=False, embedd_layer='conv4', embedd_type='L1dist_hinge', enc_layers=None, erode=False, fa=0, feats=0, feats_back=False, filts=3, fout=<_io.TextIOWrapper name='/ME/My Drive/Colab Notebooks/STVAE/OUT_conv4_emb.txt' mode='a+' encoding='UTF-8'>, full_dim=256, gpu=1, h_factor=0.2, hdim=256, hid_dataset='cifar10', hid_drop=0.0, hid_hid=256, hid_layers=['name:input', 'name:dense_final_hid;num_units:10'], hid_lr=0.001, hid_nepoch=400, hinge=False, input_channels=1, lamda=1.0, lamda1=1.0, layers=['name:input;num_filters:3', 'name:conv1;num_filters:32;filter_size:5;non_linearity:relu;nb:0', 'name:pool1;pool_size:2', '#name:drop1;drop:.5', 'name:conv2;num_filters:64;filter_size:5;non_linearity:relu;nb:0', 'name:pool2;pool_size:2', '#name:drop2;drop:.5', 'name:conv3;num_filters:128;filter_size:5;non_linearity:relu;nb:0', 'name:pool3;pool_size:2', '#name:drop3;drop:.5', 'name:conv4;num_filters:256;filter_size:3;non_linearity:relu;nb:0', 'name:dense_final;num_units:64;nb:0;parent:[conv4]'], layers_dict=[{'name': 'input', 'num_filters': 3}, {'name': 'conv1', 'num_filters': 32, 'filter_size': 5, 'non_linearity': 'relu', 'nb': 0}, {'name': 'pool1', 'pool_size': 2}, {'name': 'conv2', 'num_filters': 64, 'filter_size': 5, 'non_linearity': 'relu', 'nb': 0}, {'name': 'pool2', 'pool_size': 2}, {'name': 'conv3', 'num_filters': 128, 'filter_size': 5, 'non_linearity': 'relu', 'nb': 0}, {'name': 'pool3', 'pool_size': 2}, {'name': 'conv4', 'num_filters': 256, 'filter_size': 3, 'non_linearity': 'relu', 'nb': 0}, {'name': 'dense_final', 'num_units': 64, 'nb': 0, 'parent': ['conv4']}], layerwise=True, lim=0, lnti={'input': 0, 'conv1': 1, 'pool1': 2, 'conv2': 3, 'pool2': 4, 'conv3': 5, 'pool3': 6, 'conv4': 7, 'dense_final': 8}, lr=0.001, mb_size=500, model=['network_pool3_emb'], model_out='network_conv4_emb', mu_lr=[0.05, 0.01], n_class=0, n_mix=0, n_part_locs=0, n_parts=0, nepoch=400, network=True, network_num_train=50000, nti=500, num_class=10, num_mu_iter=10, num_test=0, num_train=50000, nval=0, nvi=20, only_pi=False, opt_jump=1, optimizer='Adam', ortho_lr=0.0, out_file='OUT_conv4_emb.txt', output_cont=False, output_prefix='', par_file='t_par', part_dim=None, perturb=0.4, pool=2, pool_stride=2, reinit=True, rerun=False, run_existing=True, s_factor=4.0, sample=False, scale=None, sched=0, sdim=26, seed=1111, sep=False, tps_num=3, transformation='aff', type='net', update_layers=None, wd=0)\n","input [   1 4096]\n","dense_final_hid [ 1 10]\n","ModuleList(\n","  (dense_final_hid): FALinear(in_features=4096, out_features=10, bias=True)\n",")\n","layers.dense_final_hid.weight,[  10 4096]\n","layers.dense_final_hid.weight_fb,[  10 4096]\n","layers.dense_final_hid.bias,[10]\n","tot_pars,81930\n","TO optimizer layers.dense_final_hid.weight[  10 4096]\n","TO optimizer layers.dense_final_hid.weight_fb[  10 4096]\n","TO optimizer layers.dense_final_hid.bias[10]\n","Optimizer Adam 0.001\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"fqC7YF6Eqhgv","colab_type":"code","colab":{}},"source":["#import tex\n","reload(tex)\n","tex.make_tex('save/')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"lJL57cJ4_IYg","colab_type":"code","colab":{}},"source":["#logi=[[False, 0], [False, 1], [False, 2],\n","#      [True, 0], [True, 1], [True, 2]]\n","\n","#cori=[True,False]\n","fname='pars_emb_cifar'\n","cori=[True]\n","logi=[[True,0]]\n","for ll in logi:\n","  for cco in cori:\n","    copy_to_content(fname,predir)\n","    if ll[0]:\n","      os.system(\"echo --layerwise > temp.txt\")\n","    os.system(\"echo --fa=\"+str(ll[1])+\" >>temp.txt\")\n","    if cco:\n","      os.system(\"echo --hinge>> temp.txt\")\n","    os.system(\"cat \"+fname+\".txt >> temp.txt\")\n","    os.system(\"mv temp.txt \"+fname+\".txt\")\n","    #os.system(\"cp pars_big_cl.txt try\"+str(ll[0])+str(ll[1])+str(cco)+\".txt\")\n","    seq(fname,predir,device)\n","    "],"execution_count":null,"outputs":[]}]}