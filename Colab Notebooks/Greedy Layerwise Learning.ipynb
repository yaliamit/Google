{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Greedy Layerwise Learning.ipynb","provenance":[],"collapsed_sections":["kRpUqf27iHUt","Yos1-W2fMmPg","YacOl4HGwkqX","ypxmyp3yY9QE","5COFlwDSMvN3","GX0IWAaCW48A","oOIEWc4Vm5ju","mbKH8e6TZHYM"],"toc_visible":true},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"KB0bqRI-3h6B","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":54},"executionInfo":{"status":"ok","timestamp":1596985952906,"user_tz":300,"elapsed":299,"user":{"displayName":"Yali Amit","photoUrl":"","userId":"06843479468473845794"}},"outputId":"04d64647-998e-4aec-ee9c-67604e9a8faf"},"source":["from google.colab import drive\n","import os, sys\n","if 'Linux' in os.uname():\n","    from google.colab import drive\n","    drive.mount('/ME')\n","    predir='/ME/My Drive/'\n","else:\n","    predir='/Users/amit/Google Drive/'\n","datadirs=predir+'Colab Notebooks/FA/'\n","datadirsa=predir+'Colab Notebooks/STVAE/_CODE'\n","\n","sys.path.insert(1, datadirs)\n","sys.path.insert(1, datadirsa)\n","savepath = datadirs+'save/'\n","datapath = predir+'LSDA_data'"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Drive already mounted at /ME; to attempt to forcibly remount, call drive.mount(\"/ME\", force_remount=True).\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"6GEsesMf3uCt","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":69},"executionInfo":{"status":"ok","timestamp":1596985977589,"user_tz":300,"elapsed":4127,"user":{"displayName":"Yali Amit","photoUrl":"","userId":"06843479468473845794"}},"outputId":"eaacf6da-6248-4bc6-e1b7-fa81c51eab46"},"source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import torch.optim as optim\n","from pprint import pprint\n","# layers\n","from layers import FALinear, FAConv2d, UfLinear, UfConv2d, UsLinear, UsConv2d\n","# Module classes and training and testing routines\n","from utils import *\n","from networks import *\n","device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n","print(device)\n","dtype = torch.float32\n","# data\n","data = get_cifar10_man(datapath, num_train=45000)"],"execution_count":2,"outputs":[{"output_type":"stream","text":["cuda:0\n","Files already downloaded and verified\n","Files already downloaded and verified\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"4w3QwbHWhCXl","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1596985981351,"user_tz":300,"elapsed":389,"user":{"displayName":"Yali Amit","photoUrl":"","userId":"06843479468473845794"}}},"source":["class pars:\n","    def __init__(self):\n","\n","      self.OPT='SGD'\n","      self.layerwise=True\n","      self.device=device\n","      self.fa=True\n","      self.START_LAYER = 0\n","      self.NUM_LAYERS = 4\n","      self.NUM_CHANNEL = 32 \n","      self.HW = 32\n","      self.epochs = 200\n","      self.batch_size= 500\n","      self.MX=1.\n","      self.LR=[.1,.05,.025,.0125]\n","      self.CR='CE'\n","      self.get_net=get_net_c\n","\n","pars=pars()\n","class layer_pars:\n","    def __init__(self,pars):\n","      self.NCOLD=3\n","      self.NC=pars.NUM_CHANNEL\n","      self.HW=pars.HW\n","pars.layer_pars=layer_pars(pars)\n"],"execution_count":3,"outputs":[]},{"cell_type":"code","metadata":{"id":"nz7X5ehM3AN7","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":1000},"outputId":"ff8adc72-b837-441a-d52b-f4642845d75a"},"source":["logi=[[False, False],[True, False], [True, True]]\n","cori=['CE','H']\n","\n","for ll in logi:\n","  for cc in cori:\n","    pars.layerwise=ll[0]\n","    pars.fa=ll[1]\n","    pars.CR=cc\n","    pars.layer_pars=layer_pars(pars)\n","    get_net_pre(pars)\n","    run_net(data,pars,savepath)\n","    show_results(pars, savepath)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["{'CR': 'CE',\n"," 'HW': 32,\n"," 'LR': [0.1, 0.05, 0.025, 0.0125],\n"," 'MX': 1.0,\n"," 'NUM_CHANNEL': 32,\n"," 'NUM_LAYERS': 4,\n"," 'OPT': 'SGD',\n"," 'START_LAYER': 0,\n"," 'batch_size': 500,\n"," 'criterion': CrossEntropyLoss(),\n"," 'device': device(type='cuda', index=0),\n"," 'epochs': 200,\n"," 'fa': False,\n"," 'get_net': <function get_net_c at 0x7fb94d9d2e18>,\n"," 'layer_pars': <__main__.layer_pars object at 0x7fb9458bb4e0>,\n"," 'layerwise': False}\n","\n","\n","\n","LAYER:3\n","Sequential(\n","  (layer0): Sequential(\n","    (0): Conv2d(3, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n","    (1): Hardtanh(min_val=0, max_val=1.0)\n","    (2): Dropout(p=0.5, inplace=False)\n","    (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n","  )\n","  (layer1): Sequential(\n","    (0): Conv2d(32, 128, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n","    (1): Hardtanh(min_val=0, max_val=1.0)\n","    (2): Dropout(p=0.5, inplace=False)\n","    (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n","  )\n","  (layer2): Sequential(\n","    (0): Conv2d(128, 512, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n","    (1): Hardtanh(min_val=0, max_val=1.0)\n","    (2): Dropout(p=0.5, inplace=False)\n","    (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n","  )\n",")\n","Sequential(\n","  (layer): Sequential(\n","    (0): Conv2d(512, 2048, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n","    (1): Hardtanh(min_val=0, max_val=1.0)\n","    (2): Dropout(p=0.5, inplace=False)\n","    (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n","  )\n","  (aux): Sequential(\n","    (0): AvgPool2d(kernel_size=1, stride=1, padding=0)\n","    (1): Flatten()\n","    (2): Linear(in_features=8192, out_features=10, bias=True)\n","  )\n",")\n","SGD (\n","Parameter Group 0\n","    dampening: 0\n","    lr: 0.1\n","    momentum: 0\n","    nesterov: False\n","    weight_decay: 0\n",")\n","Epoch 0, loss = 1.8619, val.acc = 0.3416\n","\n","Epoch 1, loss = 1.6758, val.acc = 0.4078\n","\n","Epoch 2, loss = 1.4987, val.acc = 0.4724\n","\n","Epoch 3, loss = 1.4643, val.acc = 0.5072\n","\n","Epoch 4, loss = 1.3241, val.acc = 0.5446\n","\n","Epoch 5, loss = 1.2940, val.acc = 0.5622\n","\n","Epoch 6, loss = 1.1578, val.acc = 0.6106\n","\n","Epoch 7, loss = 1.1448, val.acc = 0.5994\n","\n","Epoch 8, loss = 1.0694, val.acc = 0.6170\n","\n","Epoch 9, loss = 1.1001, val.acc = 0.6658\n","\n","Epoch 10, loss = 0.9107, val.acc = 0.6596\n","\n","Epoch 11, loss = 0.8524, val.acc = 0.6872\n","\n","Epoch 12, loss = 0.8346, val.acc = 0.6886\n","\n","Epoch 13, loss = 0.8021, val.acc = 0.6938\n","\n","Epoch 14, loss = 0.7471, val.acc = 0.7008\n","\n","Epoch 15, loss = 0.6823, val.acc = 0.7154\n","\n","Epoch 16, loss = 0.7164, val.acc = 0.6974\n","\n","Epoch 17, loss = 0.5877, val.acc = 0.7402\n","\n","Epoch 18, loss = 0.6030, val.acc = 0.7232\n","\n","Epoch 19, loss = 0.5954, val.acc = 0.7234\n","\n","Epoch 20, loss = 0.5204, val.acc = 0.7446\n","\n","Epoch 21, loss = 0.4881, val.acc = 0.7494\n","\n","Epoch 22, loss = 0.4491, val.acc = 0.7564\n","\n","Epoch 23, loss = 0.4189, val.acc = 0.7548\n","\n","Epoch 24, loss = 0.4208, val.acc = 0.7584\n","\n","Epoch 25, loss = 0.3727, val.acc = 0.7520\n","\n","Epoch 26, loss = 0.3490, val.acc = 0.7598\n","\n","Epoch 27, loss = 0.3714, val.acc = 0.7500\n","\n","Epoch 28, loss = 0.3214, val.acc = 0.7676\n","\n","Epoch 29, loss = 0.3241, val.acc = 0.7570\n","\n","Epoch 30, loss = 0.2946, val.acc = 0.7714\n","\n","Epoch 31, loss = 0.2763, val.acc = 0.7532\n","\n","Epoch 32, loss = 0.2739, val.acc = 0.7522\n","\n","Epoch 33, loss = 0.2123, val.acc = 0.7626\n","\n","Epoch 34, loss = 0.2398, val.acc = 0.7516\n","\n","Epoch 35, loss = 0.1695, val.acc = 0.7746\n","\n","Epoch 36, loss = 0.1569, val.acc = 0.7792\n","\n","Epoch 37, loss = 0.2736, val.acc = 0.7632\n","\n","Epoch 38, loss = 0.1670, val.acc = 0.7672\n","\n","Epoch 39, loss = 0.1510, val.acc = 0.7644\n","\n","Epoch 40, loss = 0.1227, val.acc = 0.7734\n","\n","Epoch 41, loss = 0.1018, val.acc = 0.7816\n","\n","Epoch 42, loss = 0.1062, val.acc = 0.7770\n","\n","Epoch 43, loss = 0.0736, val.acc = 0.7826\n","\n","Epoch 44, loss = 0.1228, val.acc = 0.7830\n","\n","Epoch 45, loss = 0.0867, val.acc = 0.7758\n","\n","Epoch 46, loss = 0.0793, val.acc = 0.7782\n","\n","Epoch 47, loss = 0.0761, val.acc = 0.7928\n","\n","Epoch 48, loss = 0.0674, val.acc = 0.7914\n","\n","Epoch 49, loss = 0.0830, val.acc = 0.7796\n","\n","Epoch 50, loss = 0.0834, val.acc = 0.7910\n","\n","Epoch 51, loss = 0.0534, val.acc = 0.7890\n","\n","Epoch 52, loss = 0.0531, val.acc = 0.7878\n","\n","Epoch 53, loss = 0.0366, val.acc = 0.7924\n","\n","Epoch 54, loss = 0.0609, val.acc = 0.7920\n","\n","Epoch 55, loss = 0.0427, val.acc = 0.7964\n","\n","Epoch 56, loss = 0.0600, val.acc = 0.7804\n","\n","Epoch 57, loss = 0.0444, val.acc = 0.7890\n","\n","Epoch 58, loss = 0.0419, val.acc = 0.7956\n","\n","Epoch 59, loss = 0.0384, val.acc = 0.7960\n","\n","Epoch 60, loss = 0.0330, val.acc = 0.7978\n","\n","Epoch 61, loss = 0.0353, val.acc = 0.7880\n","\n","Epoch 62, loss = 0.0322, val.acc = 0.7926\n","\n","Epoch 63, loss = 0.0322, val.acc = 0.7926\n","\n","Epoch 64, loss = 0.0399, val.acc = 0.7866\n","\n","Epoch 65, loss = 0.0318, val.acc = 0.7940\n","\n","Epoch 66, loss = 0.0326, val.acc = 0.7846\n","\n","Epoch 67, loss = 0.0530, val.acc = 0.8008\n","\n","Epoch 68, loss = 0.0333, val.acc = 0.7948\n","\n","Epoch 69, loss = 0.0388, val.acc = 0.7920\n","\n","Epoch 70, loss = 0.0417, val.acc = 0.7980\n","\n","Epoch 71, loss = 0.0255, val.acc = 0.8010\n","\n","Epoch 72, loss = 0.0265, val.acc = 0.7976\n","\n","Epoch 73, loss = 0.0222, val.acc = 0.8026\n","\n","Epoch 74, loss = 0.0194, val.acc = 0.8032\n","\n","Epoch 75, loss = 0.0261, val.acc = 0.7962\n","\n","Epoch 76, loss = 0.0223, val.acc = 0.7976\n","\n","Epoch 77, loss = 0.0224, val.acc = 0.7946\n","\n","Epoch 78, loss = 0.0169, val.acc = 0.7996\n","\n","Epoch 79, loss = 0.0157, val.acc = 0.7994\n","\n","Epoch 80, loss = 0.0167, val.acc = 0.8008\n","\n","Epoch 81, loss = 0.0226, val.acc = 0.8028\n","\n","Epoch 82, loss = 0.0167, val.acc = 0.8002\n","\n","Epoch 83, loss = 0.0195, val.acc = 0.7994\n","\n","Epoch 84, loss = 0.0202, val.acc = 0.7998\n","\n","Epoch 85, loss = 0.0222, val.acc = 0.8048\n","\n","Epoch 86, loss = 0.0179, val.acc = 0.8004\n","\n","Epoch 87, loss = 0.0239, val.acc = 0.7984\n","\n","Epoch 88, loss = 0.0137, val.acc = 0.8036\n","\n","Epoch 89, loss = 0.0158, val.acc = 0.8014\n","\n","Epoch 90, loss = 0.0228, val.acc = 0.7996\n","\n","Epoch 91, loss = 0.0192, val.acc = 0.7998\n","\n","Epoch 92, loss = 0.0182, val.acc = 0.8008\n","\n","Epoch 93, loss = 0.0151, val.acc = 0.7986\n","\n","Epoch 94, loss = 0.0128, val.acc = 0.7970\n","\n","Epoch 95, loss = 0.0124, val.acc = 0.7988\n","\n","Epoch 96, loss = 0.0154, val.acc = 0.7952\n","\n","Epoch 97, loss = 0.0206, val.acc = 0.8046\n","\n","Epoch 98, loss = 0.0166, val.acc = 0.8012\n","\n","Epoch 99, loss = 0.0093, val.acc = 0.8050\n","\n"],"name":"stdout"}]}]}