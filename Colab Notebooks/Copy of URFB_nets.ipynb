{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Copy of URFB_nets.ipynb","provenance":[{"file_id":"1wx120h-utrLxeZeDji971hP_YhwQvDU5","timestamp":1594828585369},{"file_id":"1hZsjD2a_JRPkX1UIdoHHo0muo_rN2lBL","timestamp":1594412247304}],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU","widgets":{"application/vnd.jupyter.widget-state+json":{"2e917c9fdebe4f0d9290e8efc481b8dd":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_00d7b38a6fd94eebba6a432468048076","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_a22f321dc2274a93873ea91ba5eee4e1","IPY_MODEL_d12bb63e617f45efaef4df028c1a8043"]}},"00d7b38a6fd94eebba6a432468048076":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"a22f321dc2274a93873ea91ba5eee4e1":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","state":{"_view_name":"ProgressView","style":"IPY_MODEL_49d5f217a0e74143816bbd36d96d6ad5","_dom_classes":[],"description":"","_model_name":"FloatProgressModel","bar_style":"info","max":1,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":1,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_36cada468f08458f85049b20cb8bd76f"}},"d12bb63e617f45efaef4df028c1a8043":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_view_name":"HTMLView","style":"IPY_MODEL_75a20340bddc48f4926a8ae4b1704c6d","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 170500096/? [00:20&lt;00:00, 34715595.35it/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_a0582d6892774ea697c585e4c51da18c"}},"49d5f217a0e74143816bbd36d96d6ad5":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"36cada468f08458f85049b20cb8bd76f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"75a20340bddc48f4926a8ae4b1704c6d":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"a0582d6892774ea697c585e4c51da18c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"b69fd24b619c4866813ea59b745d3b20":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_762a7f48bc9c40cfb2c8dfe593166682","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_0facebc50fec4a38a616f76076a5cadf","IPY_MODEL_60730c02105247fb9553bb9e4955a906"]}},"762a7f48bc9c40cfb2c8dfe593166682":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"0facebc50fec4a38a616f76076a5cadf":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","state":{"_view_name":"ProgressView","style":"IPY_MODEL_453b079960bc4e648142b1d94f506b26","_dom_classes":[],"description":"","_model_name":"FloatProgressModel","bar_style":"info","max":1,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":1,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_03f978daf8a845d7a89232966b9c6aca"}},"60730c02105247fb9553bb9e4955a906":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_view_name":"HTMLView","style":"IPY_MODEL_19a20d5a4cc64dbd84043496181ba01e","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 170500096/? [00:20&lt;00:00, 28820850.46it/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_83d09cd0fd2444678e7cdb30ee2f38d4"}},"453b079960bc4e648142b1d94f506b26":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"03f978daf8a845d7a89232966b9c6aca":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"19a20d5a4cc64dbd84043496181ba01e":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"83d09cd0fd2444678e7cdb30ee2f38d4":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}}}}},"cells":[{"cell_type":"code","metadata":{"id":"eB9X6Ehpy29q","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":121},"executionInfo":{"status":"ok","timestamp":1594276527783,"user_tz":300,"elapsed":16966,"user":{"displayName":"Ethne Yang","photoUrl":"","userId":"00321264058976725553"}},"outputId":"98b32da1-5b90-46c8-ff3d-921fdc976998"},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n","\n","Enter your authorization code:\n","··········\n","Mounted at /content/drive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"-7tyDXz_0Mkc","colab_type":"code","colab":{}},"source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","from torch.autograd import Function\n","\n","import numpy as np"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"dWrpDPoDPqDb","colab_type":"text"},"source":["# UfLinear Layer"]},{"cell_type":"code","metadata":{"id":"oU3UCmtBPsno","colab_type":"code","colab":{}},"source":["class UfLinearFunc(Function):\n","\n","    # Note that both forward and backward are @staticmethods\n","    @staticmethod\n","    # bias is an optional argument\n","    def forward(ctx, input, weight, weight_fb, bias=None):\n","        ctx.save_for_backward(input, weight, weight_fb, bias)\n","        output = input.mm(weight.t())\n","        if bias is not None:\n","            output += bias.unsqueeze(0).expand_as(output)\n","        return output\n","\n","    # This function has only a single output, so it gets only one gradient\n","    @staticmethod\n","    def backward(ctx, grad_output):\n","        # This is a pattern that is very convenient - at the top of backward\n","        # unpack saved_tensors and initialize all gradients w.r.t. inputs to\n","        # None. Thanks to the fact that additional trailing Nones are\n","        # ignored, the return statement is simple even when the function has\n","        # optional inputs.\n","        input, weight, weight_fb, bias = ctx.saved_tensors\n","        grad_input = grad_weight = grad_weight_fb = grad_bias = None\n","\n","        # These needs_input_grad checks are optional and there only to\n","        # improve efficiency. If you want to make your code simpler, you can\n","        # skip them. Returning gradients for inputs that don't require it is\n","        # not an error.\n","        if ctx.needs_input_grad[0]:\n","            grad_input = grad_output.mm(weight_fb) #weight_fb\n","        if ctx.needs_input_grad[1]:\n","            grad_weight = grad_output.t().mm(input)\n","        if ctx.needs_input_grad[2]:\n","            grad_weight_fb = grad_weight\n","        if bias is not None and ctx.needs_input_grad[3]:\n","            grad_bias = grad_output.sum(0)\n","\n","        return grad_input, grad_weight, grad_weight_fb, grad_bias"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"DqZrvq5TQp_b","colab_type":"code","colab":{}},"source":["import math\n","from torch import Tensor\n","\n","class UfLinear(nn.Module):\n","    \n","    __constants__ = ['in_features', 'out_features']\n","    in_features: int\n","    out_features: int\n","    weight: Tensor\n","\n","    def __init__(self, in_features: int, out_features: int, bias: bool = True) -> None:\n","        super(UfLinear, self).__init__()\n","        self.in_features = in_features\n","        self.out_features = out_features\n","        self.weight = nn.Parameter(torch.Tensor(out_features, in_features))\n","        self.weight_fb = nn.Parameter(torch.Tensor(out_features, in_features)) # feedbak weight\n","        if bias:\n","            self.bias = nn.Parameter(torch.Tensor(out_features))\n","        else:\n","            self.register_parameter('bias', None)\n","        self.reset_parameters()\n","\n","    def reset_parameters(self) -> None:\n","        nn.init.kaiming_uniform_(self.weight, a=math.sqrt(5))\n","        nn.init.kaiming_uniform_(self.weight_fb, a=math.sqrt(5)) # feedback weight\n","        if self.bias is not None:\n","            fan_in, _ = nn.init._calculate_fan_in_and_fan_out(self.weight)\n","            bound = 1 / math.sqrt(fan_in)\n","            nn.init.uniform_(self.bias, -bound, bound)\n","\n","    def forward(self, input: Tensor) -> Tensor:\n","        return UfLinearFunc.apply(input, self.weight, self.weight_fb, self.bias)\n","\n","    def extra_repr(self) -> str:\n","        return 'in_features={}, out_features={}, bias={}'.format(\n","            self.in_features, self.out_features, self.bias is not None\n","        )"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"kasVUnR0ztOE","colab_type":"text"},"source":["# UfConv2d Layer"]},{"cell_type":"code","metadata":{"id":"lNrOrzKazrEe","colab_type":"code","colab":{}},"source":["class UfConv2dFunc(Function):\n","\n","    # Note that both forward and backward are @staticmethods\n","    @staticmethod\n","    def forward(ctx, input, weight, weight_fb, bias=None, stride=1, padding=0, dilation=1, groups=1):\n","        ctx.save_for_backward(input, weight, weight_fb, bias) # Add weight for backward\n","        ctx.stride = stride\n","        ctx.padding = padding \n","        ctx.dilation = dilation\n","        ctx.groups = groups\n","\n","        output = F.conv2d(input, weight, bias, stride, padding, dilation, groups)\n","        return output\n","\n","    # This function has only a single output, so it gets only one gradient\n","    @staticmethod\n","    def backward(ctx, grad_output):\n","        # This is a pattern that is very convenient - at the top of backward\n","        # unpack saved_tensors and initialize all gradients w.r.t. inputs to\n","        # None. Thanks to the fact that additional trailing Nones are\n","        # ignored, the return statement is simple even when the function has\n","        # optional inputs.\n","        input, weight, weight_fb, bias = ctx.saved_tensors # Weight for backward\n","        stride = ctx.stride\n","        padding = ctx.padding \n","        dilation = ctx.dilation\n","        groups = ctx.groups\n","\n","        grad_input = grad_weight = grad_weight_fb = grad_bias = None\n","\n","        # These needs_input_grad checks are optional and there only to\n","        # improve efficiency. If you want to make your code simpler, you can\n","        # skip them. Returning gradients for inputs that don't require it is\n","        # not an error.\n","        if ctx.needs_input_grad[0]: ## use weight_fb\n","            grad_input = torch.nn.grad.conv2d_input(input.shape, weight_fb, grad_output, stride, padding, dilation, groups)\n","        if ctx.needs_input_grad[1]:\n","            grad_weight = torch.nn.grad.conv2d_weight(input, weight.shape, grad_output, stride, padding, dilation, groups)\n","        if ctx.needs_input_grad[2]:\n","            grad_weight_fb = grad_weight\n","        if bias is not None and ctx.needs_input_grad[3]:\n","            grad_bias = grad_output.sum((0,2,3))\n","\n","        return grad_input, grad_weight, grad_weight_fb, grad_bias, None, None, None, None "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"D8r2yDB0z3h8","colab_type":"code","colab":{}},"source":["from torch.nn.modules.conv import _ConvNd\n","from torch.nn.modules.utils import _pair\n","\n","# For initialization\n","import math\n","\n","class UfConv2d(_ConvNd):\n","    def __init__(self, in_channels, out_channels, kernel_size, stride=1,\n","             padding=0, dilation=1, groups=1, bias=True, padding_mode='zeros'):\n","        kernel_size = _pair(kernel_size)\n","        stride = _pair(stride)\n","        padding = _pair(padding)\n","        dilation = _pair(dilation)\n","        super(UfConv2d, self).__init__(in_channels, out_channels, kernel_size, stride, padding, dilation,\n","             False, _pair(0), groups, bias, padding_mode)\n","        self.weight_fb = nn.Parameter(torch.Tensor(\n","                out_channels, in_channels // groups, *kernel_size))\n","        #Initialize\n","        #self.weight_fb = self.weight # Same as normal backprop\n","        nn.init.kaiming_uniform_(self.weight_fb, a=math.sqrt(5))\n","        \n","    def forward(self, input):\n","        if self.padding_mode != 'zeros':\n","            return UfConv2dFunc.apply(F.pad(input, self._reversed_padding_repeated_twice, mode=self.padding_mode),\n","                            self.weight, self.weight_fb, self.bias, self.stride, _pair(0), self.dilation, self.groups)\n","        return UfConv2dFunc.apply(input, self.weight, self.weight_fb, self.bias, self.stride,\n","                        self.padding, self.dilation, self.groups)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Ei3hHFX5y8Lj","colab_type":"text"},"source":["# Data"]},{"cell_type":"code","metadata":{"id":"m0hBWKMLrDc6","colab_type":"code","colab":{}},"source":["from torch.utils.data import DataLoader\n","from torch.utils.data import sampler\n","\n","import torchvision.datasets as dset\n","import torchvision.transforms as T"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"mvdC8_kRHo_E","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":67},"executionInfo":{"status":"ok","timestamp":1594256994395,"user_tz":300,"elapsed":9067,"user":{"displayName":"Ethne Yang","photoUrl":"","userId":"00321264058976725553"}},"outputId":"58d32b7a-438a-499a-a86d-e8186901f15e"},"source":["# Compute mean and std of dataset\n","\n","transform = T.Compose([T.ToTensor()])\n","dataset = dset.CIFAR10(root='./data', train=True, download=True, transform=transform)\n","loader = DataLoader(dataset,batch_size=10,shuffle=False)\n","\n","mean = 0.\n","std = 0.\n","for images, _ in loader: # batch_size * channel * H * W\n","    batch_samples = images.size(0) # batch size (the last batch can have smaller size!)\n","    images = images.view(batch_samples, images.size(1), -1)\n","    mean += images.mean(2).sum(0)\n","    std += images.std(2).sum(0)\n","\n","mean /= len(loader.dataset)\n","std /= len(loader.dataset)\n","print('mean:',mean)\n","print('std:',std)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Files already downloaded and verified\n","mean: tensor([0.4914, 0.4822, 0.4465])\n","std: tensor([0.2023, 0.1994, 0.2010])\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"Q5ztu0gX0CHe","colab_type":"code","colab":{}},"source":["def get_cifar10(num_train, batch_size):\n","    transform = T.Compose(\n","        [T.ToTensor(),\n","         T.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n","    \n","    # transform = T.Compose([\n","    #             T.ToTensor(),\n","    #             T.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))\n","    #         ])\n","\n","    trainset = dset.CIFAR10(root='./data', train=True,\n","                            download=True, transform=transform)\n","    loader_train =DataLoader(trainset, batch_size=batch_size,\n","                             sampler=sampler.SubsetRandomSampler(range(num_train)))\n","    \n","    valset = dset.CIFAR10(root='./data', train=True,\n","                            download=True, transform=transform)\n","    loader_val =DataLoader(valset, batch_size=batch_size,\n","                             sampler=sampler.SubsetRandomSampler(range(num_train, 50000)))\n","\n","    testset = dset.CIFAR10(root='./data', train=False,\n","                           download=True, transform=transform)\n","    loader_test = DataLoader(testset, batch_size=batch_size,\n","                             shuffle=False, num_workers=2)\n","    \n","    return loader_train, loader_val, loader_test"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"x5bEo739VpnW","colab_type":"text"},"source":["# Train and test"]},{"cell_type":"code","metadata":{"id":"6LXPOX20VrSW","colab_type":"code","colab":{}},"source":["def train_model(model, optimizer, criterion, epochs=1):\n","    \"\"\"\n","    Train a model on CIFAR-10 using the PyTorch Module API.\n","    \n","    Inputs:\n","    - model: A PyTorch Module giving the model to train.\n","    - optimizer: An Optimizer object we will use to train the model\n","    - epochs: (Optional) A Python integer giving the number of epochs to train for\n","    \n","    Returns: Nothing, but prints model accuracies during training.\n","    \"\"\"\n","    model = model.to(device=device)  # move the model parameters to CPU/GPU\n","    for e in range(epochs):\n","        for t, (x, y) in enumerate(loader_train):\n","            model.train()  # put model to training mode\n","            x = x.to(device=device, dtype=dtype)  # move to device, e.g. GPU\n","            y = y.to(device=device, dtype=torch.long)\n","\n","            scores = model(x)\n","            loss = criterion(scores, y)\n","\n","            # Zero out all of the gradients for the variables which the optimizer\n","            # will update.\n","            optimizer.zero_grad()\n","\n","            # This is the backwards pass: compute the gradient of the loss with\n","            # respect to each  parameter of the model.\n","            loss.backward()\n","\n","            # Actually update the parameters of the model using the gradients\n","            # computed by the backwards pass.\n","            optimizer.step()\n","\n","            #if t % print_every == 0:\n","        print('Epoch %d, loss = %.4f' % (e, loss.item()))\n","        check_accuracy(loader_val, model)\n","        print()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"cCFVKpVQzKpI","colab_type":"code","colab":{}},"source":["def check_accuracy(loader, model):\n","    if loader.dataset.train:\n","        print('Checking accuracy on validation set')\n","    else:\n","        print('Checking accuracy on test set')   \n","    num_correct = 0\n","    num_samples = 0\n","    model.eval()  # set model to evaluation mode\n","    with torch.no_grad():\n","        for x, y in loader:\n","            x = x.to(device=device, dtype=dtype)  # move to device, e.g. GPU\n","            y = y.to(device=device, dtype=torch.long)\n","            scores = model(x)\n","            _, preds = scores.max(1)\n","            num_correct += (preds == y).sum()\n","            num_samples += preds.size(0)\n","        acc = float(num_correct) / num_samples\n","        print('Got %d / %d correct (%.2f)' % (num_correct, num_samples, 100 * acc))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"o2rSrzBMz8Z6","colab_type":"text"},"source":["# Simpnet"]},{"cell_type":"code","metadata":{"id":"rgkwxGpmz-tp","colab_type":"code","colab":{}},"source":["class Cifar10_Simpnet(nn.Module):\n","    def __init__(self):\n","        super().__init__()\n","        self.conv1 = UfConv2d(3, 32, kernel_size=5, padding=2)\n","        self.drop1 = nn.Dropout2d(p=0.8)\n","        self.fc1 = UfLinear(32*16*16, 500)\n","        self.drop2 = nn.Dropout2d(p=0.3)\n","        self.fc2 = UfLinear(500, 10)\n","\n","    def forward(self, x): # 3*32*32\n","        x = torch.tanh(self.conv1(x)) # 32*32*32\n","        x = F.max_pool2d(x, 2, stride=2) # 32*16*16\n","        x = self.drop1(x)\n","        x = x.view(-1,32*16*16)\n","        x = torch.tanh(self.fc1(x))\n","        x = self.drop2(x)\n","        x = self.fc2(x)\n","\n","        return x"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"7njr3SLq9sXt","colab_type":"code","colab":{}},"source":["class Cifar10_Simpnet_bp(nn.Module):\n","    def __init__(self):\n","        super().__init__()\n","        self.conv1 = nn.Conv2d(3, 32, kernel_size=5, padding=2)\n","        self.drop1 = nn.Dropout2d(p=0.8)\n","        self.fc1 = nn.Linear(32*16*16, 500)\n","        self.drop2 = nn.Dropout2d(p=0.3)\n","        self.fc2 = nn.Linear(500, 10)\n","\n","    def forward(self, x): # 3*32*32\n","        x = torch.tanh(self.conv1(x)) # 32*32*32\n","        x = F.max_pool2d(x, 2, stride=2) # 32*16*16\n","        x = self.drop1(x)\n","        x = x.view(-1,32*16*16)\n","        x = torch.tanh(self.fc1(x))\n","        x = self.drop2(x)\n","        x = self.fc2(x)\n","\n","        return x"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"fkBBs6ivMCPe","colab_type":"text"},"source":["# Test on Cifar-10"]},{"cell_type":"code","metadata":{"id":"bTAIFRgNUVT4","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"executionInfo":{"status":"ok","timestamp":1594403828189,"user_tz":300,"elapsed":852,"user":{"displayName":"Ethne Yang","photoUrl":"","userId":"00321264058976725553"}},"outputId":"dc77eaa9-5e53-4b81-e2ad-beb24f73e227"},"source":["device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n","print(device)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["cuda:0\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"cIX6UH_i3J1K","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":116,"referenced_widgets":["2e917c9fdebe4f0d9290e8efc481b8dd","00d7b38a6fd94eebba6a432468048076","a22f321dc2274a93873ea91ba5eee4e1","d12bb63e617f45efaef4df028c1a8043","49d5f217a0e74143816bbd36d96d6ad5","36cada468f08458f85049b20cb8bd76f","75a20340bddc48f4926a8ae4b1704c6d","a0582d6892774ea697c585e4c51da18c"]},"executionInfo":{"status":"ok","timestamp":1594403841664,"user_tz":300,"elapsed":11514,"user":{"displayName":"Ethne Yang","photoUrl":"","userId":"00321264058976725553"}},"outputId":"c76826fe-6396-4073-edf2-b2a48957604e"},"source":["loader_train, loader_val, loader_test = get_cifar10(num_train=45000, batch_size=256)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./data/cifar-10-python.tar.gz\n"],"name":"stdout"},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"2e917c9fdebe4f0d9290e8efc481b8dd","version_minor":0,"version_major":2},"text/plain":["HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["Extracting ./data/cifar-10-python.tar.gz to ./data\n","Files already downloaded and verified\n","Files already downloaded and verified\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"pSptMCKN-vqX","colab_type":"code","colab":{}},"source":["import torch.optim as optim\n","\n","#print_every=10000\n","dtype = torch.float32 "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"L5GkxMFyMB00","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":722},"executionInfo":{"status":"ok","timestamp":1594403978457,"user_tz":300,"elapsed":107555,"user":{"displayName":"Ethne Yang","photoUrl":"","userId":"00321264058976725553"}},"outputId":"6fe780ee-d8e3-47e2-ee7a-1036e04acfd0"},"source":["net = Cifar10_Simpnet()\n","criterion = nn.CrossEntropyLoss()\n","optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)\n","train_model(net, optimizer, criterion, epochs=10)\n","check_accuracy(loader_test, net)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Epoch 0, loss = 2.0461\n","Checking accuracy on validation set\n","Got 1371 / 5000 correct (27.42)\n","\n","Epoch 1, loss = 1.9524\n","Checking accuracy on validation set\n","Got 1624 / 5000 correct (32.48)\n","\n","Epoch 2, loss = 1.8990\n","Checking accuracy on validation set\n","Got 1815 / 5000 correct (36.30)\n","\n","Epoch 3, loss = 1.8398\n","Checking accuracy on validation set\n","Got 1919 / 5000 correct (38.38)\n","\n","Epoch 4, loss = 1.8166\n","Checking accuracy on validation set\n","Got 1967 / 5000 correct (39.34)\n","\n","Epoch 5, loss = 1.7633\n","Checking accuracy on validation set\n","Got 2028 / 5000 correct (40.56)\n","\n","Epoch 6, loss = 1.7793\n","Checking accuracy on validation set\n","Got 2082 / 5000 correct (41.64)\n","\n","Epoch 7, loss = 1.7713\n","Checking accuracy on validation set\n","Got 2122 / 5000 correct (42.44)\n","\n","Epoch 8, loss = 1.7002\n","Checking accuracy on validation set\n","Got 2158 / 5000 correct (43.16)\n","\n","Epoch 9, loss = 1.6918\n","Checking accuracy on validation set\n","Got 2196 / 5000 correct (43.92)\n","\n","Checking accuracy on test set\n","Got 4417 / 10000 correct (44.17)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"1eWPOz_I91wS","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":756},"executionInfo":{"status":"ok","timestamp":1594271308837,"user_tz":300,"elapsed":131187,"user":{"displayName":"Ethne Yang","photoUrl":"","userId":"00321264058976725553"}},"outputId":"9443963d-71a5-4bf8-d9c2-e124ce0f65c7"},"source":["net = Cifar10_Simpnet_bp()\n","criterion = nn.CrossEntropyLoss()\n","optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)\n","\n","train_model(net, optimizer, criterion, epochs=10)\n","check_accuracy(loader_test, net)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:1558: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n","  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n"],"name":"stderr"},{"output_type":"stream","text":["Epoch 0, loss = 1.9647\n","Checking accuracy on validation set\n","Got 1716 / 5000 correct (34.32)\n","\n","Epoch 1, loss = 1.9136\n","Checking accuracy on validation set\n","Got 1907 / 5000 correct (38.14)\n","\n","Epoch 2, loss = 1.8111\n","Checking accuracy on validation set\n","Got 1958 / 5000 correct (39.16)\n","\n","Epoch 3, loss = 1.9075\n","Checking accuracy on validation set\n","Got 2045 / 5000 correct (40.90)\n","\n","Epoch 4, loss = 1.7348\n","Checking accuracy on validation set\n","Got 2124 / 5000 correct (42.48)\n","\n","Epoch 5, loss = 1.7541\n","Checking accuracy on validation set\n","Got 2175 / 5000 correct (43.50)\n","\n","Epoch 6, loss = 1.7852\n","Checking accuracy on validation set\n","Got 2218 / 5000 correct (44.36)\n","\n","Epoch 7, loss = 1.7507\n","Checking accuracy on validation set\n","Got 2244 / 5000 correct (44.88)\n","\n","Epoch 8, loss = 1.7109\n","Checking accuracy on validation set\n","Got 2283 / 5000 correct (45.66)\n","\n","Epoch 9, loss = 1.6705\n","Checking accuracy on validation set\n","Got 2302 / 5000 correct (46.04)\n","\n","Checking accuracy on test set\n","Got 4553 / 10000 correct (45.53)\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"3G6gQ7BPLgmZ","colab_type":"text"},"source":["# UsLayer"]},{"cell_type":"code","metadata":{"id":"3B-ydCOCLibi","colab_type":"code","colab":{}},"source":["class UsLinearFunc(Function):\n","\n","    # Note that both forward and backward are @staticmethods\n","    @staticmethod\n","    # bias is an optional argument\n","    def forward(ctx, input, weight, weight_fb, bias=None):\n","        ctx.save_for_backward(input, weight, weight_fb, bias)\n","        output = input.mm(weight.t())\n","        if bias is not None:\n","            output += bias.unsqueeze(0).expand_as(output)\n","        return output\n","\n","    # This function has only a single output, so it gets only one gradient\n","    @staticmethod\n","    def backward(ctx, grad_output):\n","        # This is a pattern that is very convenient - at the top of backward\n","        # unpack saved_tensors and initialize all gradients w.r.t. inputs to\n","        # None. Thanks to the fact that additional trailing Nones are\n","        # ignored, the return statement is simple even when the function has\n","        # optional inputs.\n","        input, weight, weight_fb, bias = ctx.saved_tensors\n","        grad_input = grad_weight = grad_weight_fb = grad_bias = None\n","\n","        # These needs_input_grad checks are optional and there only to\n","        # improve efficiency. If you want to make your code simpler, you can\n","        # skip them. Returning gradients for inputs that don't require it is\n","        # not an error.\n","        if ctx.needs_input_grad[0]:\n","            grad_input = grad_output.mm(torch.sign(weight_fb))\n","        if ctx.needs_input_grad[1]:\n","            grad_weight = grad_output.t().mm(input)\n","        if ctx.needs_input_grad[2]:\n","            grad_weight_fb = grad_weight\n","        if bias is not None and ctx.needs_input_grad[3]:\n","            grad_bias = grad_output.sum(0)\n","\n","        return grad_input, grad_weight, grad_weight_fb, grad_bias"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"9aUWF37xL7XL","colab_type":"code","colab":{}},"source":["import math\n","from torch import Tensor\n","\n","class UsLinear(nn.Module):\n","    \n","    __constants__ = ['in_features', 'out_features']\n","    in_features: int\n","    out_features: int\n","    weight: Tensor\n","\n","    def __init__(self, in_features: int, out_features: int, bias: bool = True) -> None:\n","        super(UsLinear, self).__init__()\n","        self.in_features = in_features\n","        self.out_features = out_features\n","        self.weight = nn.Parameter(torch.Tensor(out_features, in_features))\n","        self.weight_fb = nn.Parameter(torch.Tensor(out_features, in_features)) # feedbak weight\n","        if bias:\n","            self.bias = nn.Parameter(torch.Tensor(out_features))\n","        else:\n","            self.register_parameter('bias', None)\n","        self.reset_parameters()\n","\n","    def reset_parameters(self) -> None:\n","        nn.init.kaiming_uniform_(self.weight, a=math.sqrt(5))\n","        nn.init.kaiming_uniform_(self.weight_fb, a=math.sqrt(5)) # feedback weight\n","        if self.bias is not None:\n","            fan_in, _ = nn.init._calculate_fan_in_and_fan_out(self.weight)\n","            bound = 1 / math.sqrt(fan_in)\n","            nn.init.uniform_(self.bias, -bound, bound)\n","\n","    def forward(self, input: Tensor) -> Tensor:\n","        return UsLinearFunc.apply(input, self.weight, self.weight_fb, self.bias)\n","\n","    def extra_repr(self) -> str:\n","        return 'in_features={}, out_features={}, bias={}'.format(\n","            self.in_features, self.out_features, self.bias is not None\n","        )"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Bu751m3JMGK4","colab_type":"code","colab":{}},"source":["class UsConv2dFunc(Function):\n","\n","    # Note that both forward and backward are @staticmethods\n","    @staticmethod\n","    def forward(ctx, input, weight, weight_fb, bias=None, stride=1, padding=0, dilation=1, groups=1):\n","        ctx.save_for_backward(input, weight, weight_fb, bias) # Add weight for backward\n","        ctx.stride = stride\n","        ctx.padding = padding \n","        ctx.dilation = dilation\n","        ctx.groups = groups\n","\n","        output = F.conv2d(input, weight, bias, stride, padding, dilation, groups)\n","        return output\n","\n","    # This function has only a single output, so it gets only one gradient\n","    @staticmethod\n","    def backward(ctx, grad_output):\n","        # This is a pattern that is very convenient - at the top of backward\n","        # unpack saved_tensors and initialize all gradients w.r.t. inputs to\n","        # None. Thanks to the fact that additional trailing Nones are\n","        # ignored, the return statement is simple even when the function has\n","        # optional inputs.\n","        input, weight, weight_fb, bias = ctx.saved_tensors # Weight for backward\n","        stride = ctx.stride\n","        padding = ctx.padding \n","        dilation = ctx.dilation\n","        groups = ctx.groups\n","\n","        grad_input = grad_weight = grad_weight_fb = grad_bias = None\n","\n","        # These needs_input_grad checks are optional and there only to\n","        # improve efficiency. If you want to make your code simpler, you can\n","        # skip them. Returning gradients for inputs that don't require it is\n","        # not an error.\n","        if ctx.needs_input_grad[0]: ## use weight_fb\n","            grad_input = torch.nn.grad.conv2d_input(input.shape, torch.sign(weight_fb), grad_output, stride, padding, dilation, groups)\n","        if ctx.needs_input_grad[1]:\n","            grad_weight = torch.nn.grad.conv2d_weight(input, weight.shape, grad_output, stride, padding, dilation, groups)\n","        if ctx.needs_input_grad[2]:\n","            grad_weight_fb = grad_weight\n","        if bias is not None and ctx.needs_input_grad[3]:\n","            grad_bias = grad_output.sum((0,2,3))\n","\n","        return grad_input, grad_weight, grad_weight_fb, grad_bias, None, None, None, None "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"SdEQ8mYnMIRw","colab_type":"code","colab":{}},"source":["from torch.nn.modules.conv import _ConvNd\n","from torch.nn.modules.utils import _pair\n","\n","# For initialization\n","import math\n","\n","class UsConv2d(_ConvNd):\n","    def __init__(self, in_channels, out_channels, kernel_size, stride=1,\n","             padding=0, dilation=1, groups=1, bias=True, padding_mode='zeros'):\n","        kernel_size = _pair(kernel_size)\n","        stride = _pair(stride)\n","        padding = _pair(padding)\n","        dilation = _pair(dilation)\n","        super(UsConv2d, self).__init__(in_channels, out_channels, kernel_size, stride, padding, dilation,\n","             False, _pair(0), groups, bias, padding_mode)\n","        self.weight_fb = nn.Parameter(torch.Tensor(\n","                out_channels, in_channels // groups, *kernel_size))\n","        #Initialize\n","        #self.weight_fb = self.weight # Same as normal backprop\n","        nn.init.kaiming_uniform_(self.weight_fb, a=math.sqrt(5))\n","        \n","    def forward(self, input):\n","        if self.padding_mode != 'zeros':\n","            return UsConv2dFunc.apply(F.pad(input, self._reversed_padding_repeated_twice, mode=self.padding_mode),\n","                            self.weight, self.weight_fb, self.bias, self.stride, _pair(0), self.dilation, self.groups)\n","        return UsConv2dFunc.apply(input, self.weight, self.weight_fb, self.bias, self.stride,\n","                        self.padding, self.dilation, self.groups)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"SrVdmpKNMfd-","colab_type":"text"},"source":["# Test Us"]},{"cell_type":"code","metadata":{"id":"-uqPCS3zMe8_","colab_type":"code","colab":{}},"source":["class Cifar10_Simpnet_Us(nn.Module):\n","    def __init__(self):\n","        super().__init__()\n","        self.conv1 = UsConv2d(3, 32, kernel_size=5, padding=2)\n","        self.drop1 = nn.Dropout2d(p=0.8)\n","        self.fc1 = UsLinear(32*16*16, 500)\n","        self.drop2 = nn.Dropout2d(p=0.3)\n","        self.fc2 = UsLinear(500, 10)\n","\n","    def forward(self, x): # 3*32*32\n","        x = torch.tanh(self.conv1(x)) # 32*32*32\n","        x = F.max_pool2d(x, 2, stride=2) # 32*16*16\n","        x = self.drop1(x)\n","        x = x.view(-1,32*16*16)\n","        x = torch.tanh(self.fc1(x))\n","        x = self.drop2(x)\n","        x = self.fc2(x)\n","\n","        return x"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"uvsg2ilwMsQz","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":133,"referenced_widgets":["b69fd24b619c4866813ea59b745d3b20","762a7f48bc9c40cfb2c8dfe593166682","0facebc50fec4a38a616f76076a5cadf","60730c02105247fb9553bb9e4955a906","453b079960bc4e648142b1d94f506b26","03f978daf8a845d7a89232966b9c6aca","19a20d5a4cc64dbd84043496181ba01e","83d09cd0fd2444678e7cdb30ee2f38d4"]},"executionInfo":{"status":"ok","timestamp":1594392623369,"user_tz":300,"elapsed":11479,"user":{"displayName":"Ethne Yang","photoUrl":"","userId":"00321264058976725553"}},"outputId":"24e755e6-5721-4275-aa64-9e18e83c6679"},"source":["device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n","print(device)\n","loader_train, loader_val, loader_test = get_cifar10(num_train=45000, batch_size=256)\n","\n","import torch.optim as optim\n","dtype = torch.float32 "],"execution_count":null,"outputs":[{"output_type":"stream","text":["cuda:0\n","Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./data/cifar-10-python.tar.gz\n"],"name":"stdout"},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"b69fd24b619c4866813ea59b745d3b20","version_minor":0,"version_major":2},"text/plain":["HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["Extracting ./data/cifar-10-python.tar.gz to ./data\n","Files already downloaded and verified\n","Files already downloaded and verified\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"xtQUpS4yNEUh","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":722},"executionInfo":{"status":"ok","timestamp":1594404122631,"user_tz":300,"elapsed":96511,"user":{"displayName":"Ethne Yang","photoUrl":"","userId":"00321264058976725553"}},"outputId":"13c15cf8-1cea-48d7-9c8a-3abb9a43dcf9"},"source":["net = Cifar10_Simpnet_Us()\n","criterion = nn.CrossEntropyLoss()\n","optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)\n","train_model(net, optimizer, criterion, epochs=10)\n","check_accuracy(loader_test, net)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Epoch 0, loss = 1.9228\n","Checking accuracy on validation set\n","Got 1886 / 5000 correct (37.72)\n","\n","Epoch 1, loss = 1.8127\n","Checking accuracy on validation set\n","Got 2062 / 5000 correct (41.24)\n","\n","Epoch 2, loss = 1.7918\n","Checking accuracy on validation set\n","Got 2172 / 5000 correct (43.44)\n","\n","Epoch 3, loss = 1.8184\n","Checking accuracy on validation set\n","Got 2256 / 5000 correct (45.12)\n","\n","Epoch 4, loss = 1.7719\n","Checking accuracy on validation set\n","Got 2282 / 5000 correct (45.64)\n","\n","Epoch 5, loss = 1.7273\n","Checking accuracy on validation set\n","Got 2354 / 5000 correct (47.08)\n","\n","Epoch 6, loss = 1.6187\n","Checking accuracy on validation set\n","Got 2372 / 5000 correct (47.44)\n","\n","Epoch 7, loss = 1.7044\n","Checking accuracy on validation set\n","Got 2409 / 5000 correct (48.18)\n","\n","Epoch 8, loss = 1.6402\n","Checking accuracy on validation set\n","Got 2459 / 5000 correct (49.18)\n","\n","Epoch 9, loss = 1.6932\n","Checking accuracy on validation set\n","Got 2437 / 5000 correct (48.74)\n","\n","Checking accuracy on test set\n","Got 4918 / 10000 correct (49.18)\n"],"name":"stdout"}]}]}